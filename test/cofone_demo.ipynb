{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0001",
   "metadata": {},
   "source": [
    "# ğŸ§  Cofone â€” Complete Feature Demo\n",
    "\n",
    "> **Simple, fast, yours.** Turn documents, websites and videos into a queryable knowledge base in a few lines of Python.\n",
    "\n",
    "This notebook covers **every single feature** of the `cofone` library:\n",
    "\n",
    "| # | Section |\n",
    "|---|---|\n",
    "| 0 | Setup & installation |\n",
    "| 1 | Basic usage |\n",
    "| 2 | Debug mode |\n",
    "| 3 | Chunking modes (4 modes) |\n",
    "| 4 | Retrieval â€” BM25 vs FAISS |\n",
    "| 5 | FAISS persistence to disk |\n",
    "| 6 | All 19 LLM providers |\n",
    "| 7 | All 10 embedding providers |\n",
    "| 8 | All source types (file, folder, PDF, web, Wikipedia, YouTube) |\n",
    "| 9 | Multiple sources combined |\n",
    "| 10 | Chat memory |\n",
    "| 11 | Streaming |\n",
    "| 12 | Structured output (Pydantic) |\n",
    "| 13 | Custom tools |\n",
    "| 14 | API key configuration (3 ways) |\n",
    "| 15 | Power combos â€” advanced patterns |\n",
    "| 16 | Full parameter reference |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002",
   "metadata": {},
   "source": [
    "---\n",
    "## 0 Â· Setup\n",
    "\n",
    "Install cofone with all optional extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install everything\n",
    "# pip install \"cofone[all]\"\n",
    "\n",
    "# Or pick only what you need:\n",
    "# pip install cofone              # base: txt, md, BM25, OpenRouter\n",
    "# pip install \"cofone[pdf]\"       # adds PDF support (pypdf)\n",
    "# pip install \"cofone[faiss]\"     # adds FAISS + sentence-transformers\n",
    "# pip install \"cofone[web]\"       # adds Wikipedia + YouTube\n",
    "# pip install \"cofone[all]\"       # everything above\n",
    "\n",
    "print(\"Installation command: pip install 'cofone[all]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Standard imports used throughout this notebook â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from cofone import RAG\n",
    "\n",
    "# Load API keys from .env file in the project root\n",
    "# The .env file should contain lines like:\n",
    "#   OPENROUTER_API_KEY=sk-or-...\n",
    "#   OPENAI_API_KEY=sk-...\n",
    "#   ANTHROPIC_API_KEY=sk-ant-...\n",
    "#   GEMINI_API_KEY=AI...\n",
    "#   etc.\n",
    "load_dotenv(Path().resolve().parent / '.env')\n",
    "\n",
    "# Base path for local test files\n",
    "BASE = Path().resolve()\n",
    "\n",
    "print(f\"BASE path: {BASE}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 Â· Basic usage\n",
    "\n",
    "The simplest possible usage: load a file, ask a question, get an answer.\n",
    "\n",
    "```\n",
    "RAG()                        # create the RAG pipeline\n",
    "  .add_source(\"file.txt\")   # load a document\n",
    "  .run(\"question\")           # ask a question â†’ returns a string\n",
    "```\n",
    "\n",
    "By default:\n",
    "- LLM provider: **OpenRouter** (free tier, no credit card needed)\n",
    "- Model: `arcee-ai/trinity-large-preview:free`\n",
    "- Retrieval: **BM25** (keyword search, no extra deps)\n",
    "- Chunking: **smart** (paragraph-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal example â€” everything at default\n",
    "answer = RAG().add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fluent one-liner â€” chain everything\n",
    "print(RAG().add_source(BASE / 'note_ex.txt').run('What is the main topic of this document?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask multiple questions on the same source\n",
    "src = RAG().add_source(BASE / 'note_ex.txt')\n",
    "\n",
    "print(src.run('Give me a 2-sentence summary.'))\n",
    "print()\n",
    "print(src.run('What are the key points?'))\n",
    "print()\n",
    "print(src.run('What questions does this document raise?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 Â· Debug mode\n",
    "\n",
    "`.debug()` enables verbose logging. It prints:\n",
    "- Provider and model being used\n",
    "- Number of documents loaded\n",
    "- Number of chunks created\n",
    "- Which chunks were retrieved for the query (with text preview)\n",
    "- Token estimate\n",
    "\n",
    "Always useful during development to understand what's happening inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug mode â€” insert .debug() anywhere in the chain\n",
    "answer = (\n",
    "    RAG()\n",
    "    .debug()                            # â† enables verbose logging\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .run('Summarize')\n",
    ")\n",
    "print()\n",
    "print('ANSWER:', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug with FAISS â€” shows embedding model and vector dimensions\n",
    "answer = (\n",
    "    RAG(faiss=True)\n",
    "    .debug()\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .run('What is this about?')\n",
    ")\n",
    "print()\n",
    "print('ANSWER:', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 Â· Chunking modes\n",
    "\n",
    "Chunking = how text is split into pieces before indexing.\n",
    "The chunk is the unit of retrieval â€” a smaller, focused piece of text sent to the LLM.\n",
    "\n",
    "| Mode | Strategy | Best for |\n",
    "|---|---|---|\n",
    "| `smart` | paragraphs â†’ sentences if >600 chars | articles, docs, Wikipedia |\n",
    "| `paragraphs` | split on blank lines only | short self-contained paragraphs |\n",
    "| `sentences` | split on `.!?`, group up to ~500 chars | legal, scientific, dense text |\n",
    "| `fixed` | 500-char slices with 50-char overlap | raw data, logs, unstructured text |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smart (default) â€” paragraph-aware, breaks long paragraphs into sentences\n",
    "# Algorithm:\n",
    "#   1. Split by blank lines â†’ paragraphs\n",
    "#   2. If a paragraph > 600 chars â†’ split into sentences\n",
    "#   3. Group sentences until ~500 chars total, then start a new chunk\n",
    "answer = RAG(chunk_mode='smart').add_source(BASE / 'note_ex.txt').run('What is this about?')\n",
    "print('[smart]', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraphs â€” splits ONLY on blank lines (\\n\\n)\n",
    "# Each paragraph becomes exactly one chunk, regardless of its length.\n",
    "# Warning: if a paragraph is very long, the chunk may exceed context window.\n",
    "answer = RAG(chunk_mode='paragraphs').add_source(BASE / 'note_ex.txt').run('What is this about?')\n",
    "print('[paragraphs]', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences â€” splits on . ! ? boundaries, then groups to ~500 chars\n",
    "# Finest granularity â€” every sentence is important.\n",
    "# Good for: legal documents, scientific papers, technical manuals.\n",
    "answer = RAG(chunk_mode='sentences').add_source(BASE / 'note_ex.txt').run('What is this about?')\n",
    "print('[sentences]', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed â€” exactly 500-char slices with 50-char overlap between consecutive chunks\n",
    "# The overlap prevents losing context at boundaries.\n",
    "# Good for: log files, CSV dumps, unstructured raw text.\n",
    "answer = RAG(chunk_mode='fixed').add_source(BASE / 'note_ex.txt').run('What is this about?')\n",
    "print('[fixed]', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chunk counts â€” see how many pieces each mode creates\n",
    "print('Chunk counts for note_ex.txt:')\n",
    "print('-' * 40)\n",
    "for mode in ['smart', 'paragraphs', 'sentences', 'fixed']:\n",
    "    rag = RAG(chunk_mode=mode).add_source(BASE / 'note_ex.txt')\n",
    "    n = len(rag._retriever.chunks)\n",
    "    print(f'  [{mode:12s}] â†’ {n:3d} chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the actual chunks created by each mode\n",
    "print('First 3 chunks with mode=smart:')\n",
    "print('=' * 60)\n",
    "rag = RAG(chunk_mode='smart').add_source(BASE / 'note_ex.txt')\n",
    "for i, chunk in enumerate(rag._retriever.chunks[:3]):\n",
    "    print(f'--- Chunk {i+1} ({len(chunk)} chars) ---')\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 Â· Retrieval â€” BM25 vs FAISS\n",
    "\n",
    "### BM25 (default)\n",
    "Keyword-based. Ranks chunks by term frequency Ã— inverse document frequency.\n",
    "- âœ… Zero dependencies, instant, great for exact term matches\n",
    "- âŒ \"automobile\" won't match a chunk about \"cars\"\n",
    "\n",
    "### FAISS â€” semantic search\n",
    "Vector-based. Converts chunks and query to dense vectors, retrieves by geometric proximity.\n",
    "- âœ… Finds synonyms, paraphrases, conceptually related text\n",
    "- âœ… \"vehicle\" matches \"car\", \"automobile\", \"transportation\"\n",
    "- âŒ Requires `faiss-cpu` + embedding model (first run is slower)\n",
    "\n",
    "Requires: `pip install \"cofone[faiss]\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 â€” default, no extra setup needed\n",
    "print('=== BM25 (keyword matching) ===')\n",
    "answer_bm25 = (\n",
    "    RAG()\n",
    "    .debug()\n",
    "    .add_source(BASE / 'docs_ex/')\n",
    "    .run('Who is Leonardo?')\n",
    ")\n",
    "print('ANSWER:', answer_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS â€” semantic search with local sentence-transformers embeddings\n",
    "# Default embedding: all-MiniLM-L6-v2 (fast, English-focused, 384 dims)\n",
    "# No API key needed â€” runs 100% locally\n",
    "print('=== FAISS (semantic search, local embeddings) ===')\n",
    "answer_faiss = (\n",
    "    RAG(faiss=True)\n",
    "    .debug()\n",
    "    .add_source(BASE / 'docs_ex/')\n",
    "    .run('Who is Leonardo?')\n",
    ")\n",
    "print('ANSWER:', answer_faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS with a multilingual embedding model\n",
    "# paraphrase-multilingual-MiniLM-L12-v2 supports 50+ languages\n",
    "# Use this when your documents are in Italian, French, Spanish, etc.\n",
    "print('=== FAISS (multilingual embeddings) ===')\n",
    "answer_multi = (\n",
    "    RAG(\n",
    "        faiss=True,\n",
    "        embedding_provider='local',\n",
    "        embedding_model='paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    )\n",
    "    .debug()\n",
    "    .add_source(BASE / 'docs_ex/')\n",
    "    .run('Descrivi i dipinti di Leonardo')  # Italian query\n",
    ")\n",
    "print('ANSWER:', answer_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison: BM25 vs FAISS on a semantic query\n",
    "# A query using synonyms â€” BM25 may miss it, FAISS should find it\n",
    "QUERY = 'What vehicles did the Renaissance genius design?'  # won't say 'Leonardo' directly\n",
    "\n",
    "print('--- BM25 ---')\n",
    "print(RAG().add_source(BASE / 'docs_ex/').run(QUERY))\n",
    "print()\n",
    "print('--- FAISS ---')\n",
    "print(RAG(faiss=True).add_source(BASE / 'docs_ex/').run(QUERY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 Â· FAISS persistence (cache to disk)\n",
    "\n",
    "Without `persist_path`, FAISS rebuilds the entire index every run.\n",
    "For large document sets (PDFs, thousands of pages) this can take minutes.\n",
    "\n",
    "`persist_path` saves the index to disk on first run, then loads it instantly on all subsequent runs.\n",
    "\n",
    "**What gets saved:**\n",
    "```\n",
    "my_index/\n",
    "  index.faiss      â† the vector index\n",
    "  chunks.json      â† the original text chunks\n",
    "  metadata.json    â† chunk count, embedding model, dimensions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = BASE / '.cofone_cache'\n",
    "\n",
    "# FIRST RUN â€” computes embeddings and saves index to disk\n",
    "# [cofone] index saved to .cofone_cache\n",
    "print('=== First run (builds & saves) ===')\n",
    "answer = (\n",
    "    RAG(faiss=True, persist_path=db)\n",
    "    .add_source(BASE / 'docs_ex/')\n",
    "    .run('Who is Leonardo da Vinci?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND RUN â€” loads from disk instantly, no recomputation\n",
    "# [cofone] index loaded from .cofone_cache (N chunks)\n",
    "print('=== Second run (loads from disk, instant) ===')\n",
    "answer = (\n",
    "    RAG(faiss=True, persist_path=db)\n",
    "    .add_source(BASE / 'docs_ex/')\n",
    "    .run('What are his most famous inventions?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect what got saved\n",
    "import os\n",
    "if db.exists():\n",
    "    print(f'Files in {db}:')\n",
    "    for f in sorted(db.iterdir()):\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f'  {f.name:25s}  {size_kb:.1f} KB')\n",
    "else:\n",
    "    print('Cache not found â€” run the cells above first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistence with a custom embedding provider (OpenAI)\n",
    "# The metadata.json records which embedding model was used,\n",
    "# so cofone can warn you if you try to load with a different model.\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='openai',\n",
    "#     embedding_model='text-embedding-3-small',\n",
    "#     persist_path='./openai_index'\n",
    "# ).add_source('docs/').run('question')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0060",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 Â· All 19 LLM providers\n",
    "\n",
    "Cofone supports 19 LLM providers via the `model_provider` parameter.\n",
    "The interface is identical for all of them â€” swap `model_provider` and `model`, rest stays the same.\n",
    "\n",
    "### Auto-detection\n",
    "If you pass only `model=` (without `model_provider=`), cofone auto-detects the provider:\n",
    "- `gpt-*` â†’ openai\n",
    "- `gemini-*` â†’ gemini\n",
    "- `claude-*` â†’ anthropic\n",
    "- anything else â†’ openrouter (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# OPENROUTER â€” default provider\n",
    "# Key: OPENROUTER_API_KEY\n",
    "# 200+ models, free tier available, one key for everything\n",
    "# https://openrouter.ai/keys\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Default (free model, no explicit provider needed)\n",
    "RAG().add_source(BASE / 'note_ex.txt').run('Summarize in one sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenRouter â€” explicitly specify a free model\n",
    "RAG(\n",
    "    model_provider='openrouter',\n",
    "    model='meta-llama/llama-3.3-70b-instruct:free'\n",
    ").add_source(BASE / 'note_ex.txt').run('Summarize in one sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenRouter â€” another free model (Gemini via OpenRouter)\n",
    "RAG(\n",
    "    model_provider='openrouter',\n",
    "    model='google/gemini-2.0-flash-exp:free'\n",
    ").add_source(BASE / 'note_ex.txt').run('Summarize in one sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# OPENAI â€” Key: OPENAI_API_KEY\n",
    "# Models: gpt-4o-mini, gpt-4o, gpt-4-turbo, o1-mini, o3-mini, etc.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='openai',\n",
    "#     model='gpt-4o-mini'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# Auto-detection: 'gpt-*' prefix â†’ openai (no need for model_provider)\n",
    "# RAG(model='gpt-4o-mini').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='openai',\n",
    "#     model='gpt-4o'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ANTHROPIC â€” Key: ANTHROPIC_API_KEY\n",
    "# Models: claude-3-5-haiku-20241022, claude-3-5-sonnet-20241022, claude-3-opus-20240229\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='anthropic',\n",
    "#     model='claude-3-5-haiku-20241022'   # fastest & cheapest Claude\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='anthropic',\n",
    "#     model='claude-3-5-sonnet-20241022'  # best quality Claude\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# Auto-detection: 'claude-*' prefix â†’ anthropic\n",
    "# RAG(model='claude-3-5-haiku-20241022').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# GEMINI â€” Key: GEMINI_API_KEY\n",
    "# Models: gemini-2.0-flash, gemini-1.5-pro, gemini-1.5-flash\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='gemini',\n",
    "#     model='gemini-2.0-flash'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# Auto-detection: 'gemini-*' prefix â†’ gemini\n",
    "# RAG(model='gemini-2.0-flash').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MISTRAL â€” Key: MISTRAL_API_KEY\n",
    "# Models: mistral-large-latest, mistral-medium, codestral-latest\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='mistral',\n",
    "#     model='mistral-large-latest'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires MISTRAL_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# GROQ â€” Key: GROQ_API_KEY\n",
    "# Specialty: very fast inference (low latency)\n",
    "# Models: llama-3.1-8b-instant, llama-3.3-70b-versatile, mixtral-8x7b-32768\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='groq',\n",
    "#     model='llama-3.1-8b-instant'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='groq',\n",
    "#     model='llama-3.3-70b-versatile'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COHERE â€” Key: COHERE_API_KEY\n",
    "# Specialty: RAG-optimized models, great multilingual support\n",
    "# Models: command-r-plus, command-r, command\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='cohere',\n",
    "#     model='command-r-plus'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DEEPSEEK â€” Key: DEEPSEEK_API_KEY\n",
    "# Specialty: DeepSeek-R1 reasoning model (chain-of-thought)\n",
    "# Models: deepseek-chat, deepseek-reasoner\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='deepseek',\n",
    "#     model='deepseek-reasoner'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize step by step')\n",
    "\n",
    "print('Uncomment to use â€” requires DEEPSEEK_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# xAI â€” Key: XAI_API_KEY\n",
    "# Models: grok-2, grok-beta\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='xai',\n",
    "#     model='grok-2'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires XAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TOGETHER â€” Key: TOGETHER_API_KEY\n",
    "# Specialty: wide selection of open-source models\n",
    "# Models: meta-llama/Llama-3-70b-chat-hf, mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='together',\n",
    "#     model='meta-llama/Llama-3-70b-chat-hf'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires TOGETHER_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PERPLEXITY â€” Key: PERPLEXITY_API_KEY\n",
    "# Specialty: web-connected models (real-time information)\n",
    "# Models: llama-3.1-sonar-large-128k-online\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='perplexity',\n",
    "#     model='llama-3.1-sonar-large-128k-online'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires PERPLEXITY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# FIREWORKS â€” Key: FIREWORKS_API_KEY\n",
    "# Specialty: fast open-model inference\n",
    "# Models: accounts/fireworks/models/llama-v3p1-70b-instruct\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='fireworks',\n",
    "#     model='accounts/fireworks/models/llama-v3p1-70b-instruct'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires FIREWORKS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CEREBRAS â€” Key: CEREBRAS_API_KEY\n",
    "# Specialty: ultra-fast inference (dedicated silicon)\n",
    "# Models: llama3.1-8b, llama3.1-70b\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='cerebras',\n",
    "#     model='llama3.1-70b'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires CEREBRAS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NVIDIA NIM â€” Key: NVIDIA_API_KEY\n",
    "# Specialty: NVIDIA-optimized models on cloud infrastructure\n",
    "# Models: nvidia/llama-3.1-nemotron-70b-instruct\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='nvidia',\n",
    "#     model='nvidia/llama-3.1-nemotron-70b-instruct'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires NVIDIA_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DEEPINFRA â€” Key: DEEPINFRA_API_KEY\n",
    "# Specialty: cheap open-model inference\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='deepinfra',\n",
    "#     model='meta-llama/Meta-Llama-3.1-70B-Instruct'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires DEEPINFRA_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ANYSCALE â€” Key: ANYSCALE_API_KEY\n",
    "# Specialty: scalable inference with Ray-based infrastructure\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='anyscale',\n",
    "#     model='meta-llama/Llama-3-70b-chat-hf'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires ANYSCALE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# OLLAMA â€” local, no key, no internet needed\n",
    "# Install: https://ollama.com\n",
    "# Pull models: ollama pull llama3\n",
    "# Ollama must be running: ollama serve\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='ollama',\n",
    "#     model='llama3'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='ollama',\n",
    "#     model='mistral'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='ollama',\n",
    "#     model='phi3'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires Ollama running locally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# LM STUDIO â€” local, no key, no internet needed\n",
    "# Install: https://lmstudio.ai\n",
    "# Start the local server in LM Studio, then:\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='lmstudio',\n",
    "#     model='any-model-loaded-in-lmstudio'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires LM Studio with local server enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# LLAMA.CPP â€” local, no key, no internet needed\n",
    "# Install: pip install llama-cpp-python\n",
    "# Download a .gguf model and start the server:\n",
    "#   python -m llama_cpp.server --model model.gguf\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='llamacpp',\n",
    "#     model='local'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires llama.cpp server running locally')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CUSTOM OPENAI-COMPATIBLE ENDPOINT\n",
    "# Any server that speaks the OpenAI API format works via base_url=\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='openai',\n",
    "#     model='my-custom-model',\n",
    "#     base_url='http://my-server:8000/v1',\n",
    "#     model_api_key='optional-key-if-needed'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” point base_url to any OpenAI-compatible server')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 Â· All 10 embedding providers\n",
    "\n",
    "Embeddings are only used when `faiss=True`. They transform text into vectors.\n",
    "The LLM and embedding providers are **completely independent** â€” mix any combination.\n",
    "\n",
    "| Provider | `embedding_provider=` | Needs key | Notes |\n",
    "|---|---|---|---|\n",
    "| sentence-transformers | `'local'` | No | Default. Runs offline. |\n",
    "| OpenAI | `'openai'` | Yes | High quality, fast |\n",
    "| Gemini | `'gemini'` | Yes | text-embedding-004 |\n",
    "| Cohere | `'cohere'` | Yes | Multilingual specialist |\n",
    "| Mistral | `'mistral'` | Yes | mistral-embed |\n",
    "| Voyage | `'voyage'` | Yes | Top retrieval quality |\n",
    "| Jina | `'jina'` | Yes | jina-embeddings-v3 |\n",
    "| NVIDIA | `'nvidia'` | Yes | nv-embed-v2 |\n",
    "| Together | `'together'` | Yes | BGE, UAE models |\n",
    "| Ollama | `'ollama'` | No | Local, nomic-embed-text |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# LOCAL (sentence-transformers) â€” default, no key, runs offline\n",
    "# Default model: all-MiniLM-L6-v2\n",
    "#   - Fast, 384 dimensions, English-focused\n",
    "#   - Downloaded automatically on first use (~22 MB)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "RAG(\n",
    "    faiss=True,\n",
    "    embedding_provider='local',         # can also omit this â€” it's the default\n",
    "    embedding_model='all-MiniLM-L6-v2'  # can also omit this â€” it's the default\n",
    ").add_source(BASE / 'note_ex.txt').run('Summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL â€” larger model, better quality (but slower)\n",
    "# all-mpnet-base-v2: 768 dims, higher quality retrieval\n",
    "RAG(\n",
    "    faiss=True,\n",
    "    embedding_provider='local',\n",
    "    embedding_model='all-mpnet-base-v2'\n",
    ").add_source(BASE / 'note_ex.txt').run('Summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL â€” multilingual model (50+ languages)\n",
    "# Use this for Italian, French, Spanish, German, etc. documents\n",
    "RAG(\n",
    "    faiss=True,\n",
    "    embedding_provider='local',\n",
    "    embedding_model='paraphrase-multilingual-MiniLM-L12-v2'\n",
    ").add_source(BASE / 'note_ex.txt').run('Di cosa parla questo testo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# OPENAI embeddings â€” Key: OPENAI_API_KEY\n",
    "# text-embedding-3-small: fast, cheap, 1536 dims\n",
    "# text-embedding-3-large: best quality, 3072 dims\n",
    "# text-embedding-ada-002: legacy, 1536 dims\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='openai',\n",
    "#     embedding_model='text-embedding-3-small'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='openai',\n",
    "#     embedding_model='text-embedding-3-large'  # higher quality, higher cost\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# GEMINI embeddings â€” Key: GEMINI_API_KEY\n",
    "# text-embedding-004: 768 dims\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='gemini',\n",
    "#     embedding_model='text-embedding-004'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COHERE embeddings â€” Key: COHERE_API_KEY\n",
    "# Extra: pip install cohere\n",
    "# Specialty: multilingual, great for non-English documents\n",
    "# Models: embed-multilingual-v3.0, embed-english-v3.0\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='cohere',\n",
    "#     embedding_model='embed-multilingual-v3.0'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires COHERE_API_KEY + pip install cohere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MISTRAL embeddings â€” Key: MISTRAL_API_KEY\n",
    "# Models: mistral-embed (1024 dims)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='mistral',\n",
    "#     embedding_model='mistral-embed'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires MISTRAL_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# VOYAGE â€” Key: VOYAGE_API_KEY\n",
    "# Extra: pip install voyageai\n",
    "# Specialty: consistently top-ranked retrieval quality in benchmarks\n",
    "# Models: voyage-3, voyage-3-lite, voyage-code-3\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='voyage',\n",
    "#     embedding_model='voyage-3'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires VOYAGE_API_KEY + pip install voyageai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# JINA â€” Key: JINA_API_KEY\n",
    "# Models: jina-embeddings-v3 (supports multilingual, 8192 token context)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='jina',\n",
    "#     embedding_model='jina-embeddings-v3'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires JINA_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NVIDIA embeddings â€” Key: NVIDIA_API_KEY\n",
    "# Models: nvidia/nv-embed-v2\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='nvidia',\n",
    "#     embedding_model='nvidia/nv-embed-v2'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires NVIDIA_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# TOGETHER embeddings â€” Key: TOGETHER_API_KEY\n",
    "# Models: BAAI/bge-large-en-v1.5, WhereIsAI/UAE-Large-V1\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='together',\n",
    "#     embedding_model='BAAI/bge-large-en-v1.5'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires TOGETHER_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# OLLAMA embeddings â€” local, no key, no internet needed\n",
    "# Default model: nomic-embed-text\n",
    "# Pull: ollama pull nomic-embed-text\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='ollama',\n",
    "#     embedding_model='nomic-embed-text'\n",
    "# ).add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "print('Uncomment to use â€” requires Ollama running locally + nomic-embed-text pulled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 Â· All source types\n",
    "\n",
    "All sources are loaded via `.add_source()`. Cofone auto-detects the type from the path or URL.\n",
    "\n",
    "| Source | Auto-detected from |\n",
    "|---|---|\n",
    "| `.txt` / `.md` file | file extension |\n",
    "| Folder | path ends with `/` or is a directory |\n",
    "| PDF | `.pdf` extension |\n",
    "| Wikipedia | `wikipedia.org` in URL |\n",
    "| YouTube | `youtube.com` or `youtu.be` in URL |\n",
    "| Generic web URL | any other URL |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Single .txt file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "answer = RAG().add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "print('[.txt]', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Folder â€” loads all .txt, .md, .pdf files recursively â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Every file in docs_ex/ and all its subfolders gets loaded into a single index\n",
    "answer = RAG().add_source(BASE / 'docs_ex/').run('What are all the topics covered in these documents?')\n",
    "print('[folder]', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ .md (Markdown) file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Treated same as .txt â€” plain text extraction\n",
    "answer = RAG().add_source(BASE / 'README.md').run('What does this project do?')\n",
    "print('[.md]', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ PDF â€” requires pip install \"cofone[pdf]\" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Uses pypdf internally for text extraction\n",
    "\n",
    "# RAG().add_source('report.pdf').run('What are the key findings?')\n",
    "# RAG().add_source('contracts/').run('What are the payment terms?')  # folder of PDFs\n",
    "\n",
    "print('Uncomment to use â€” requires pip install \"cofone[pdf]\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Wikipedia â€” requires pip install \"cofone[web]\" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Auto-detected from wikipedia.org URL\n",
    "# Returns clean article text (no markup, no nav, no sidebars)\n",
    "# Supports any language: en.wikipedia, it.wikipedia, fr.wikipedia, etc.\n",
    "\n",
    "answer = (\n",
    "    RAG()\n",
    "    .debug()\n",
    "    .add_source('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "    .run('What is the definition of AI and when was the field founded?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia in Italian\n",
    "answer = (\n",
    "    RAG()\n",
    "    .add_source('https://it.wikipedia.org/wiki/Leonardo_da_Vinci')\n",
    "    .run('Dove nacque Leonardo da Vinci?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Generic web URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Fetches the page HTML, strips nav/footer/scripts, keeps only visible text\n",
    "\n",
    "answer = (\n",
    "    RAG()\n",
    "    .add_source('https://docs.python.org/3/library/pathlib.html')\n",
    "    .run('How do I create a Path object and check if a file exists?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ YouTube transcript â€” requires pip install \"cofone[web]\" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Fetches official subtitles/transcript (auto-generated captions also work)\n",
    "# Language priority: Italian first, then English\n",
    "\n",
    "answer = (\n",
    "    RAG()\n",
    "    .debug()\n",
    "    .add_source('https://www.youtube.com/watch?v=jNQXAC9IVRw')  # \"Me at the zoo\" (first YouTube video)\n",
    "    .run('What is this video about?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube â€” short URL format also works\n",
    "# answer = RAG().add_source('https://youtu.be/jNQXAC9IVRw').run('Summarize')\n",
    "print('Both youtube.com and youtu.be short URLs are supported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120",
   "metadata": {},
   "source": [
    "---\n",
    "## 9 Â· Multiple sources combined\n",
    "\n",
    "Chain as many `.add_source()` calls as you want.\n",
    "All documents are merged into a single index. Retrieval searches across all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine a local file + a folder\n",
    "answer = (\n",
    "    RAG()\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .add_source(BASE / 'docs_ex/')\n",
    "    .run('What do you know about Cofone and Leonardo?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine local files + Wikipedia + YouTube\n",
    "answer = (\n",
    "    RAG()\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .add_source('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "    .run('Give me a complete overview combining all sources')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug multiple sources â€” see total chunk count from all sources\n",
    "rag = (\n",
    "    RAG()\n",
    "    .debug()\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .add_source(BASE / 'docs_ex/')\n",
    ")\n",
    "print('Total chunks:', len(rag._retriever.chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130",
   "metadata": {},
   "source": [
    "---\n",
    "## 10 Â· Chat memory\n",
    "\n",
    "Two ways to enable memory:\n",
    "1. Use `.chat()` instead of `.run()` â€” automatically enables memory\n",
    "2. Pass `memory=True` to the constructor â€” keeps memory across `.run()` calls\n",
    "\n",
    "Memory injects the full conversation history into every prompt:\n",
    "```\n",
    "Conversation so far:\n",
    "User: Who is Leonardo da Vinci?\n",
    "Assistant: He was a Renaissance polymath...\n",
    "\n",
    "User: When was he born?\n",
    "Assistant: He was born on April 15, 1452...\n",
    "\n",
    "Context:\n",
    "[retrieved chunks]\n",
    "\n",
    "User: What are his most famous works?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: use .chat() â€” automatic memory, simplest API\n",
    "bot = RAG().add_source(BASE / 'docs_ex/')\n",
    "\n",
    "r1 = bot.chat('Who is Leonardo da Vinci?')\n",
    "print('Q1:', r1)\n",
    "print()\n",
    "\n",
    "r2 = bot.chat('When was he born?')          # 'he' refers to Leonardo â€” context is preserved\n",
    "print('Q2:', r2)\n",
    "print()\n",
    "\n",
    "r3 = bot.chat('What are his most famous paintings?')\n",
    "print('Q3:', r3)\n",
    "print()\n",
    "\n",
    "r4 = bot.chat('Did he have any unfinished works?')  # 'he' still = Leonardo\n",
    "print('Q4:', r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset memory â€” next call starts fresh, no previous context\n",
    "bot.reset_memory()\n",
    "\n",
    "r_fresh = bot.chat('What are we talking about?')  # no context â€” should say it doesn't know\n",
    "print('After reset:', r_fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: memory=True constructor flag â€” keeps history across .run() calls\n",
    "# Useful when you want memory but prefer the .run() interface\n",
    "bot2 = RAG(memory=True).add_source(BASE / 'docs_ex/')\n",
    "\n",
    "print(bot2.run('Who is Leonardo?'))\n",
    "print()\n",
    "print(bot2.run('What did he invent?'))   # has context from previous call\n",
    "print()\n",
    "print(bot2.run('How old was he when he started painting?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat memory + FAISS â€” full combo\n",
    "bot3 = RAG(faiss=True).add_source(BASE / 'docs_ex/')\n",
    "\n",
    "print(bot3.chat('Summarize the documents briefly.'))\n",
    "print()\n",
    "print(bot3.chat('Which part was most interesting to you?'))  # conversational follow-up\n",
    "print()\n",
    "print(bot3.chat('Tell me more about that.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sysprompt_md",
   "metadata": {},
   "source": [
    "---\n",
    "## 11 Â· System prompt\n",
    "\n",
    "The system prompt is prepended to **every** prompt and tells the LLM who it is and how to behave.\n",
    "\n",
    "Default built-in prompt:\n",
    "```\n",
    "You are a helpful assistant. Answer the user's question using ONLY the information\n",
    "provided in the context below. If the answer is not in the context, say you don't know.\n",
    "Be concise, accurate, and respond in the same language as the user's question.\n",
    "```\n",
    "Override with `system_prompt=` to set a custom role, tone, language or format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sysprompt_default",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default â€” no system_prompt needed, built-in prompt is used\n",
    "print(RAG().add_source(BASE / 'note_ex.txt').run('Summarize'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sysprompt_role",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom role â€” art historian\n",
    "print(\n",
    "    RAG(\n",
    "        system_prompt='You are an expert art historian. Answer formally and with precision.'\n",
    "    ).add_source(BASE / 'docs_ex/').run('Tell me about Leonardo')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sysprompt_language",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Italian output regardless of query language\n",
    "print(\n",
    "    RAG(\n",
    "        system_prompt='Rispondi sempre in italiano, in modo chiaro e sintetico.'\n",
    "    ).add_source(BASE / 'docs_ex/').run('What is the main topic?')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sysprompt_format",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control output format â€” always bullet points\n",
    "print(\n",
    "    RAG(\n",
    "        system_prompt='Answer always with a bullet-point list. Maximum 5 points. No prose.'\n",
    "    ).add_source(BASE / 'docs_ex/').run('Summarize the key facts about Leonardo')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sysprompt_memory_stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt + memory + streaming â€” full combo\n",
    "bot = RAG(\n",
    "    system_prompt='You are a friendly tutor. Explain concepts simply and use examples.',\n",
    "    memory=True,\n",
    "    max_history=5,\n",
    ").add_source(BASE / 'docs_ex/')\n",
    "\n",
    "for token in bot.stream('Who is Leonardo da Vinci? Explain simply.'):\n",
    "    print(token, end='', flush=True)\n",
    "print()\n",
    "print()\n",
    "for token in bot.stream('What is he most famous for?'):\n",
    "    print(token, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140",
   "metadata": {},
   "source": [
    "---\n",
    "## 12 Â· Streaming\n",
    "\n",
    "`.stream()` is a generator that yields string tokens one by one as they arrive from the LLM.\n",
    "No waiting for the full response â€” tokens print to the screen in real time.\n",
    "\n",
    "Works with all providers that support streaming (OpenRouter, OpenAI, Anthropic, Gemini, Mistral, Groq, Cohere, Ollama, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic streaming\n",
    "rag = RAG().add_source(BASE / 'docs_ex/')\n",
    "\n",
    "print('Streaming response:')\n",
    "print('-' * 50)\n",
    "for token in rag.stream(\"Tell me about Leonardo's inventions\"):\n",
    "    print(token, end='', flush=True)\n",
    "print()  # newline after streaming ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with debug â€” shows chunk info before streaming starts\n",
    "rag2 = RAG().debug().add_source(BASE / 'note_ex.txt')\n",
    "\n",
    "print('Streaming with debug:')\n",
    "print('-' * 50)\n",
    "for token in rag2.stream('Describe this document in detail'):\n",
    "    print(token, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect streamed tokens into a full string (if you need it later)\n",
    "tokens = []\n",
    "for token in RAG().add_source(BASE / 'note_ex.txt').stream('Summarize'):\n",
    "    print(token, end='', flush=True)\n",
    "    tokens.append(token)\n",
    "print()\n",
    "\n",
    "full_response = ''.join(tokens)\n",
    "print(f'\\nTotal characters received: {len(full_response)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with FAISS â€” semantic search + real-time output\n",
    "rag3 = RAG(faiss=True).add_source(BASE / 'docs_ex/')\n",
    "\n",
    "for token in rag3.stream('What is the most interesting thing about Leonardo?'):\n",
    "    print(token, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150",
   "metadata": {},
   "source": [
    "---\n",
    "## 12 Â· Structured output (Pydantic)\n",
    "\n",
    "Pass a Pydantic `BaseModel` as `schema=` to `.run()`.\n",
    "Instead of a string, you get back a **validated Python object**.\n",
    "\n",
    "How it works internally:\n",
    "1. The schema is converted to JSON Schema and appended to the prompt\n",
    "2. The LLM is instructed to respond only with a JSON object\n",
    "3. `response_format={'type': 'json_object'}` is passed where supported\n",
    "4. The response is parsed and validated with `schema.model_validate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "# Simple schema â€” extract person data\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    birth_year: int\n",
    "    nationality: str\n",
    "    most_famous_work: str\n",
    "\n",
    "data = RAG().add_source(BASE / 'docs_ex/').run(\n",
    "    'Extract data about Leonardo da Vinci',\n",
    "    schema=Person\n",
    ")\n",
    "\n",
    "print(f'Type:             {type(data)}')\n",
    "print(f'Name:             {data.name}')\n",
    "print(f'Birth year:       {data.birth_year}')\n",
    "print(f'Nationality:      {data.nationality}')\n",
    "print(f'Most famous work: {data.most_famous_work}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema with List fields\n",
    "class ArtistProfile(BaseModel):\n",
    "    name: str\n",
    "    birth_year: int\n",
    "    death_year: Optional[int]\n",
    "    nationality: str\n",
    "    main_disciplines: List[str]       # list of strings\n",
    "    notable_works: List[str]\n",
    "    historical_period: str\n",
    "\n",
    "profile = RAG().add_source(BASE / 'docs_ex/').run(\n",
    "    'Build a complete profile of Leonardo da Vinci',\n",
    "    schema=ArtistProfile\n",
    ")\n",
    "\n",
    "import json\n",
    "print(json.dumps(profile.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested schema â€” schema with nested objects\n",
    "class Work(BaseModel):\n",
    "    title: str\n",
    "    year: Optional[int]\n",
    "    category: str  # painting, sculpture, drawing, etc.\n",
    "    description: str\n",
    "\n",
    "class ArtistWorks(BaseModel):\n",
    "    artist_name: str\n",
    "    total_known_works: int\n",
    "    featured_works: List[Work]\n",
    "\n",
    "result = RAG().add_source(BASE / 'docs_ex/').run(\n",
    "    'List the most famous works of Leonardo da Vinci with details',\n",
    "    schema=ArtistWorks\n",
    ")\n",
    "\n",
    "print(f'Artist: {result.artist_name}')\n",
    "print(f'Total known works: {result.total_known_works}')\n",
    "print()\n",
    "for w in result.featured_works:\n",
    "    print(f'  [{w.year}] {w.title} ({w.category})')\n",
    "    print(f'        {w.description[:80]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema from Wikipedia source\n",
    "class AISummary(BaseModel):\n",
    "    definition: str\n",
    "    main_subfields: List[str]\n",
    "    biggest_challenge: str\n",
    "    year_field_founded: Optional[int]\n",
    "    key_researchers: List[str]\n",
    "\n",
    "result = (\n",
    "    RAG()\n",
    "    .add_source('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "    .run('Extract structured info about AI', schema=AISummary)\n",
    ")\n",
    "print(result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification schema â€” classify the document\n",
    "from typing import Literal\n",
    "\n",
    "class DocumentClassification(BaseModel):\n",
    "    document_type: Literal['article', 'manual', 'biography', 'report', 'fiction', 'other']\n",
    "    language: str\n",
    "    main_topics: List[str]\n",
    "    sentiment: Literal['positive', 'neutral', 'negative']\n",
    "    approximate_reading_time_minutes: int\n",
    "    suitable_for_ages: str  # e.g. \"all ages\", \"adults only\", \"children\"\n",
    "\n",
    "classification = RAG().add_source(BASE / 'note_ex.txt').run(\n",
    "    'Classify and analyze this document',\n",
    "    schema=DocumentClassification\n",
    ")\n",
    "print(f'Type:          {classification.document_type}')\n",
    "print(f'Language:      {classification.language}')\n",
    "print(f'Topics:        {classification.main_topics}')\n",
    "print(f'Sentiment:     {classification.sentiment}')\n",
    "print(f'Read time:     ~{classification.approximate_reading_time_minutes} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160",
   "metadata": {},
   "source": [
    "---\n",
    "## 13 Â· Custom tools\n",
    "\n",
    "Attach Python functions with `.add_tool()`. The LLM can call them during reasoning.\n",
    "Tools are described to the LLM alongside the retrieved context â€” it decides when to call them.\n",
    "\n",
    "Requirements: the function must have a **docstring** (it's used as the tool description for the LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tool: calculator\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a Python math expression and return the numeric result.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression, {'__builtins__': {}}, {})\n",
    "        return f'Result: {result}'\n",
    "    except Exception as e:\n",
    "        return f'Error: {e}'\n",
    "\n",
    "answer = (\n",
    "    RAG()\n",
    "    .add_tool(calculate)\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .run('Summarize the document and tell me what 144 divided by 12 is')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple tools attached to the same RAG\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate a Python math expression and return the numeric result.\"\"\"\n",
    "    try:\n",
    "        return f'Result: {eval(expression)}'\n",
    "    except Exception as e:\n",
    "        return f'Error: {e}'\n",
    "\n",
    "def word_count(text: str) -> str:\n",
    "    \"\"\"Count the number of words in a text string.\"\"\"\n",
    "    return f'Word count: {len(text.split())}'\n",
    "\n",
    "def reverse_text(text: str) -> str:\n",
    "    \"\"\"Reverse the characters in a string.\"\"\"\n",
    "    return text[::-1]\n",
    "\n",
    "def current_date() -> str:\n",
    "    \"\"\"Return the current date and time in ISO format.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return datetime.now().isoformat()\n",
    "\n",
    "answer = (\n",
    "    RAG()\n",
    "    .add_tool(calculate)\n",
    "    .add_tool(word_count)\n",
    "    .add_tool(reverse_text)\n",
    "    .add_tool(current_date)\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .run('What is today\\'s date? Also, what is 2 to the power of 10?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool that queries an external service (example: local filesystem)\n",
    "def list_files(folder: str) -> str:\n",
    "    \"\"\"List all files in a given folder path and return their names.\"\"\"\n",
    "    p = Path(folder)\n",
    "    if not p.exists():\n",
    "        return f'Folder {folder} does not exist.'\n",
    "    files = [f.name for f in p.iterdir() if f.is_file()]\n",
    "    return f'Files in {folder}: {files}'\n",
    "\n",
    "answer = (\n",
    "    RAG()\n",
    "    .add_tool(list_files)\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .run(f'List the files in {str(BASE)}')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170",
   "metadata": {},
   "source": [
    "---\n",
    "## 14 Â· API key configuration (3 ways)\n",
    "\n",
    "Keys are resolved in this priority order:\n",
    "1. **Direct parameter** (`model_api_key=` or `embedding_api_key=`) â€” highest priority\n",
    "2. **`.env` file** â€” recommended for development\n",
    "3. **System environment variable** â€” for CI/CD, production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Way 1: direct parameter â€” highest priority, overrides .env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# RAG(model_api_key='sk-or-...').add_source('note.txt').run('Summarize')\n",
    "\n",
    "# For embedding key separately (when faiss=True with API embedding):\n",
    "# RAG(\n",
    "#     faiss=True,\n",
    "#     embedding_provider='openai',\n",
    "#     embedding_model='text-embedding-3-small',\n",
    "#     embedding_api_key='sk-...',\n",
    "# ).add_source('note.txt').run('Summarize')\n",
    "\n",
    "# Both LLM and embedding keys directly:\n",
    "# RAG(\n",
    "#     model_provider='openai',\n",
    "#     model='gpt-4o-mini',\n",
    "#     model_api_key='sk-...',\n",
    "#     faiss=True,\n",
    "#     embedding_provider='openai',\n",
    "#     embedding_model='text-embedding-3-small',\n",
    "#     embedding_api_key='sk-...',\n",
    "# ).add_source('note.txt').run('Summarize')\n",
    "\n",
    "print('Way 1: direct parameter â€” see commented code above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Way 2: .env file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Create a file named .env in your project root with these contents:\n",
    "\n",
    "env_template = \"\"\"\n",
    "# LLM providers\n",
    "OPENROUTER_API_KEY=sk-or-...\n",
    "OPENAI_API_KEY=sk-...\n",
    "ANTHROPIC_API_KEY=sk-ant-...\n",
    "GEMINI_API_KEY=AI...\n",
    "MISTRAL_API_KEY=...\n",
    "GROQ_API_KEY=gsk_...\n",
    "COHERE_API_KEY=...\n",
    "DEEPSEEK_API_KEY=...\n",
    "XAI_API_KEY=xai-...\n",
    "TOGETHER_API_KEY=...\n",
    "FIREWORKS_API_KEY=...\n",
    "PERPLEXITY_API_KEY=pplx-...\n",
    "CEREBRAS_API_KEY=...\n",
    "NVIDIA_API_KEY=nvapi-...\n",
    "DEEPINFRA_API_KEY=...\n",
    "ANYSCALE_API_KEY=...\n",
    "\n",
    "# Embedding-only providers\n",
    "VOYAGE_API_KEY=...\n",
    "JINA_API_KEY=...\n",
    "\"\"\"\n",
    "print('Way 2: .env template:')\n",
    "print(env_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then at the top of every script/notebook:\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # reads .env from current dir and all parent dirs\n",
    "\n",
    "# Or specify path explicitly:\n",
    "load_dotenv(Path().resolve().parent / '.env')\n",
    "\n",
    "print('Keys loaded from .env (if file exists).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Way 3: system environment variable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set before running the script. Persists for the session.\n",
    "\n",
    "# PowerShell (Windows):\n",
    "# $env:OPENROUTER_API_KEY = \"sk-or-...\"\n",
    "\n",
    "# bash / zsh (Linux / macOS):\n",
    "# export OPENROUTER_API_KEY=\"sk-or-...\"\n",
    "\n",
    "# Jupyter: set in the kernel environment or use %env magic:\n",
    "# %env OPENROUTER_API_KEY=sk-or-...\n",
    "\n",
    "# Verify what's currently set:\n",
    "import os\n",
    "providers = [\n",
    "    'OPENROUTER_API_KEY', 'OPENAI_API_KEY', 'ANTHROPIC_API_KEY',\n",
    "    'GEMINI_API_KEY', 'GROQ_API_KEY', 'MISTRAL_API_KEY'\n",
    "]\n",
    "print('Current API keys status:')\n",
    "for key in providers:\n",
    "    val = os.environ.get(key, '')\n",
    "    status = 'âœ“ set' if val else 'âœ— not set'\n",
    "    print(f'  {key:<25} {status}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180",
   "metadata": {},
   "source": [
    "---\n",
    "## 15 Â· Power combos â€” advanced patterns\n",
    "\n",
    "Real-world combinations that use multiple features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMBO 1: Fully local RAG â€” no internet, no API keys, 100% private\n",
    "# Ollama LLM + Ollama embeddings + FAISS + local files\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='ollama',\n",
    "#     model='llama3',\n",
    "#     faiss=True,\n",
    "#     embedding_provider='ollama',\n",
    "#     embedding_model='nomic-embed-text'\n",
    "# ).add_source('confidential_docs/').run('Summarize')\n",
    "\n",
    "print('Combo 1: fully local â€” see commented code above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMBO 2: Max quality â€” best LLM + best embeddings + persistent index\n",
    "# GPT-4o + OpenAI embeddings + FAISS persisted to disk\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# RAG(\n",
    "#     model_provider='openai',\n",
    "#     model='gpt-4o',\n",
    "#     faiss=True,\n",
    "#     embedding_provider='openai',\n",
    "#     embedding_model='text-embedding-3-large',\n",
    "#     persist_path='./premium_index'\n",
    "# ).add_source('docs/').run('Deep analysis of all documents')\n",
    "\n",
    "print('Combo 2: max quality â€” see commented code above')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMBO 3: Chat bot with memory + streaming + FAISS\n",
    "# Conversational agent that searches semantically and streams in real time\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "bot = RAG(faiss=True, memory=True).add_source(BASE / 'docs_ex/')\n",
    "\n",
    "questions = [\n",
    "    'Who is the main subject of these documents?',\n",
    "    'What were his most important contributions?',\n",
    "    'Which of those contributions do you find most impressive and why?'\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f'\\n> {q}')\n",
    "    print('-' * 50)\n",
    "    for token in bot.stream(q):\n",
    "        print(token, end='', flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMBO 4: Structured extraction pipeline\n",
    "# Load Wikipedia + extract structured data + serialize to JSON\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "\n",
    "class TechProfile(BaseModel):\n",
    "    full_name: str\n",
    "    birth_year: Optional[int]\n",
    "    nationality: str\n",
    "    companies_founded: List[str]\n",
    "    key_inventions: List[str]\n",
    "    net_worth_usd_billions: Optional[float]\n",
    "    current_role: str\n",
    "\n",
    "profile = (\n",
    "    RAG(faiss=True)\n",
    "    .add_source('https://en.wikipedia.org/wiki/Elon_Musk')\n",
    "    .run('Extract a complete profile of Elon Musk', schema=TechProfile)\n",
    ")\n",
    "\n",
    "print(json.dumps(profile.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMBO 5: Multi-source research assistant with tools\n",
    "# Combines docs + Wikipedia + custom tools + structured output\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def today() -> str:\n",
    "    \"\"\"Return today's date in YYYY-MM-DD format.\"\"\"\n",
    "    return datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "def count_words(text: str) -> str:\n",
    "    \"\"\"Count words in a text and return the count.\"\"\"\n",
    "    return str(len(text.split()))\n",
    "\n",
    "answer = (\n",
    "    RAG(chunk_mode='smart')\n",
    "    .add_tool(today)\n",
    "    .add_tool(count_words)\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .add_source('https://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "    .run('What is today\\'s date? And summarize what Python is based on the Wikipedia article.')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMBO 6: Fast free combo â€” OpenRouter + BM25 + multiple free models compared\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "FREE_MODELS = [\n",
    "    'arcee-ai/trinity-large-preview:free',\n",
    "    'meta-llama/llama-3.3-70b-instruct:free',\n",
    "    'google/gemini-2.0-flash-exp:free',\n",
    "]\n",
    "\n",
    "QUERY = 'Summarize this document in exactly 2 sentences.'\n",
    "\n",
    "print(f'Query: {QUERY}\\n')\n",
    "for model in FREE_MODELS:\n",
    "    answer = (\n",
    "        RAG(model_provider='openrouter', model=model)\n",
    "        .add_source(BASE / 'note_ex.txt')\n",
    "        .run(QUERY)\n",
    "    )\n",
    "    print(f'[{model}]')\n",
    "    print(answer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# COMBO 7: Debug all chunking modes + both retrieval strategies\n",
    "# Benchmark how different configs affect chunk count and answer quality\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "QUERY = 'Who invented the telephone?'\n",
    "\n",
    "configs = [\n",
    "    {'name': 'BM25 + smart',      'faiss': False, 'chunk_mode': 'smart'},\n",
    "    {'name': 'BM25 + sentences',  'faiss': False, 'chunk_mode': 'sentences'},\n",
    "    {'name': 'FAISS + smart',     'faiss': True,  'chunk_mode': 'smart'},\n",
    "    {'name': 'FAISS + sentences', 'faiss': True,  'chunk_mode': 'sentences'},\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    rag = RAG(faiss=cfg['faiss'], chunk_mode=cfg['chunk_mode'])\n",
    "    rag.add_source(BASE / 'note_ex.txt')\n",
    "    n_chunks = len(rag._retriever.chunks)\n",
    "    answer = rag.run(QUERY)\n",
    "    print(f'[{cfg[\"name\"]}] â€” {n_chunks} chunks')\n",
    "    print(f'  â†’ {answer[:100]}...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190",
   "metadata": {},
   "source": [
    "---\n",
    "## 16 Â· Full parameter reference\n",
    "\n",
    "Complete documentation of every parameter and method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191",
   "metadata": {},
   "source": [
    "### `RAG()` constructor parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|---|---|---|---|\n",
    "| `model_provider` | `str` | `'openrouter'` | LLM provider. Options: `'openrouter'`, `'openai'`, `'anthropic'`, `'gemini'`, `'mistral'`, `'groq'`, `'cohere'`, `'deepseek'`, `'xai'`, `'together'`, `'perplexity'`, `'fireworks'`, `'cerebras'`, `'nvidia'`, `'deepinfra'`, `'anyscale'`, `'ollama'`, `'lmstudio'`, `'llamacpp'` |\n",
    "| `model` | `str` | provider default | LLM model identifier string |\n",
    "| `model_api_key` | `str` | `None` | LLM API key â€” overrides `.env` |\n",
    "| `base_url` | `str` | provider default | Custom LLM endpoint (any OpenAI-compatible API) |\n",
    "| `faiss` | `bool` | `False` | Use FAISS semantic search. If `False`, uses BM25 (default) |\n",
    "| `embedding_provider` | `str` | `'local'` | Embedding provider. Options: `'local'`, `'openai'`, `'gemini'`, `'cohere'`, `'mistral'`, `'voyage'`, `'jina'`, `'nvidia'`, `'together'`, `'ollama'` |\n",
    "| `embedding_model` | `str` | `'all-MiniLM-L6-v2'` | Embedding model identifier |\n",
    "| `embedding_api_key` | `str` | `None` | Embedding API key â€” overrides `.env` |\n",
    "| `chunk_mode` | `str` | `'smart'` | Chunking strategy. Options: `'smart'`, `'paragraphs'`, `'sentences'`, `'fixed'` |\n",
    "| `persist_path` | `str\\|Path` | `None` | Folder to save/load FAISS index. Only used when `faiss=True` |\n",
    "| `memory` | `bool` | `False` | Enable conversation history. Equivalent to always using `.chat()` |",
    "| `system_prompt` | `str` | `None` | Custom instructions for the LLM. `None` = built-in default |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "| Method | Returns | Description |\n",
    "|---|---|---|\n",
    "| `.add_source(path_or_url)` | `self` | Load file, folder, URL, Wikipedia, YouTube. Chainable â€” call multiple times. |\n",
    "| `.add_tool(fn)` | `self` | Attach a Python function. Function must have a docstring. |\n",
    "| `.debug()` | `self` | Enable verbose logging: provider, model, chunks, retrieval preview. |\n",
    "| `.run(query, schema=None)` | `str` or Pydantic model | Single stateless query. If `schema=` is passed, returns a validated Pydantic object. |\n",
    "| `.chat(query)` | `str` | Stateful query with automatic memory. Context accumulates across calls. |\n",
    "| `.stream(query)` | `Generator[str, None, None]` | Generator yielding string tokens one by one as they arrive from the LLM. |\n",
    "| `.reset_memory()` | `self` | Clear all conversation history. Next call starts fresh. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0193",
   "metadata": {},
   "source": [
    "### Key resolution order\n",
    "\n",
    "```\n",
    "model_api_key= parameter  â† highest priority\n",
    "  â†“ (if not set)\n",
    ".env file  (e.g. OPENROUTER_API_KEY=...)\n",
    "  â†“ (if not set)\n",
    "system environment variable\n",
    "  â†“ (if not set)\n",
    "no key needed (local providers: ollama, lmstudio, llamacpp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194",
   "metadata": {},
   "source": [
    "### Source auto-detection rules\n",
    "\n",
    "```python\n",
    "add_source(\"file.txt\")                              # â†’ plain text file\n",
    "add_source(\"README.md\")                             # â†’ markdown (treated as plain text)\n",
    "add_source(\"report.pdf\")                            # â†’ PDF (requires cofone[pdf])\n",
    "add_source(\"docs/\")                                 # â†’ folder, loads all .txt .md .pdf\n",
    "add_source(\"https://en.wikipedia.org/wiki/...\")    # â†’ Wikipedia article\n",
    "add_source(\"https://it.wikipedia.org/wiki/...\")    # â†’ Wikipedia in Italian\n",
    "add_source(\"https://www.youtube.com/watch?v=...\")  # â†’ YouTube transcript\n",
    "add_source(\"https://youtu.be/...\")                  # â†’ YouTube short URL\n",
    "add_source(\"https://any-other-url.com\")             # â†’ generic web page\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick syntax reference â€” all in one cell\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    COFONE QUICK REFERENCE                        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  MINIMAL USAGE                                                   â•‘\n",
    "â•‘  RAG().add_source(\"file.txt\").run(\"question\")                    â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  LLM PROVIDERS (19)                                              â•‘\n",
    "â•‘  openrouter* openai anthropic gemini mistral groq cohere         â•‘\n",
    "â•‘  deepseek xai together perplexity fireworks cerebras nvidia      â•‘\n",
    "â•‘  deepinfra anyscale  |  ollama* lmstudio llamacpp (local)        â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  EMBEDDING PROVIDERS (10)                                        â•‘\n",
    "â•‘  local* openai gemini cohere mistral voyage jina nvidia together â•‘\n",
    "â•‘  ollama (local)                                                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  CHUNK MODES (4)                                                 â•‘\n",
    "â•‘  smart* paragraphs sentences fixed                               â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  SOURCES                                                         â•‘\n",
    "â•‘  file.txt / README.md / report.pdf / folder/ / wikipedia URL     â•‘\n",
    "â•‘  youtube URL / any web URL / chain multiple .add_source() calls  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  METHODS                                                         â•‘\n",
    "â•‘  .run(q)           â†’ str     (stateless)                         â•‘\n",
    "â•‘  .run(q, schema=M) â†’ Pydantic (structured output)                â•‘\n",
    "â•‘  .chat(q)          â†’ str     (with memory)                       â•‘\n",
    "â•‘  .stream(q)        â†’ tokens  (real-time)                         â•‘\n",
    "â•‘  .reset_memory()   â†’ self    (clear history)                     â•‘\n",
    "â•‘  .debug()          â†’ self    (verbose logging)                   â•‘\n",
    "â•‘  .add_tool(fn)     â†’ self    (attach Python function)            â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  * = default value                                               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}