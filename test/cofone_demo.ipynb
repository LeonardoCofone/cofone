{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dc0680",
   "metadata": {},
   "source": [
    "# Cofone — Complete Feature Reference\n",
    "Every single feature of the library, in one notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from cofone import RAG\n",
    "\n",
    "load_dotenv(Path().resolve().parent / '.env')\n",
    "BASE = Path().resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c6b60",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 · Basic usage\n",
    "Load a file and ask a question. That's it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c50af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = RAG().add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d96a9b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 · Debug mode\n",
    "Shows provider, model, loaded docs, chunk count and text preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = (\n",
    "    RAG()\n",
    "    .debug()\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .run('Summarize')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47985290",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 · Chunking modes\n",
    "Control how the text is split before indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smart (default): paragraphs first; if a paragraph > 600 chars, splits into sentences\n",
    "RAG(chunk_mode='smart').add_source(BASE / 'note_ex.txt').run('What is this about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de634671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraphs: split only on blank lines\n",
    "RAG(chunk_mode='paragraphs').add_source(BASE / 'note_ex.txt').run('What is this about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences: split on . ! ?\n",
    "RAG(chunk_mode='sentences').add_source(BASE / 'note_ex.txt').run('What is this about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed: fixed-length slices (500 chars, 50 overlap)\n",
    "RAG(chunk_mode='fixed').add_source(BASE / 'note_ex.txt').run('What is this about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00450cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chunk counts across all modes\n",
    "for mode in ['smart', 'paragraphs', 'sentences', 'fixed']:\n",
    "    rag = RAG(chunk_mode=mode).add_source(BASE / 'note_ex.txt')\n",
    "    print(f'[{mode:12s}] → {len(rag._retriever.chunks)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e723f",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 · Retrieval — BM25 vs FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1577e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 (default) — keyword matching, no extra deps\n",
    "RAG().debug().add_source(BASE / 'docs_ex/').run('Who is Leonardo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS — semantic search via sentence-transformers embeddings\n",
    "RAG(faiss=True).debug().add_source(BASE / 'docs_ex/').run('Who is Leonardo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3a28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS with multilingual model — better for Italian/mixed docs\n",
    "RAG(faiss=True, embedding_model='paraphrase-multilingual-MiniLM-L12-v2') \\\n",
    "    .debug() \\\n",
    "    .add_source(BASE / 'docs_ex/') \\\n",
    "    .run('Describe the paintings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0586368b",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 · FAISS persistence (cache to disk)\n",
    "First run: builds & saves index. Second run: loads instantly, no recompute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = BASE / '.cofone_cache'\n",
    "\n",
    "# First run — slow (computes embeddings)\n",
    "RAG(faiss=True, persist_path=db).add_source(BASE / 'docs_ex/').run('Who is Leonardo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second run — instant (loads from disk)\n",
    "answer = RAG(faiss=True, persist_path=db).add_source(BASE / 'docs_ex/').run('What did Leonardo invent?')\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01850cd6",
   "metadata": {},
   "source": [
    "---\n",
    "## 6 · Providers & models\n",
    "OpenRouter, OpenAI, Gemini, Ollama — all via the same interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── OpenRouter (default) — 200+ models with one key ──────────────────────\n",
    "RAG(model='arcee-ai/trinity-large-preview:free').add_source(BASE / 'note_ex.txt').run('Summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG(model='meta-llama/llama-3.3-70b-instruct:free').add_source(BASE / 'note_ex.txt').run('Summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70332edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG(model='google/gemini-2.0-flash-exp:free').add_source(BASE / 'note_ex.txt').run('Summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── OpenAI direct — requires OPENAI_API_KEY ───────────────────────────────\n",
    "# RAG(provider='openai', model='gpt-4o-mini').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "# RAG(provider='openai', model='gpt-4o').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "# RAG(model='gpt-4o-mini').add_source(BASE / 'note_ex.txt').run('Summarize')  # auto-detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5214cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Gemini direct — requires GEMINI_API_KEY ───────────────────────────────\n",
    "# RAG(provider='gemini', model='gemini-2.0-flash').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "# RAG(model='gemini-2.0-flash').add_source(BASE / 'note_ex.txt').run('Summarize')  # auto-detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa1bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Ollama local — no key, must be running on localhost ───────────────────\n",
    "# RAG(provider='ollama', model='llama3').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "# RAG(provider='ollama', model='mistral').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "# RAG(provider='ollama', model='phi3').add_source(BASE / 'note_ex.txt').run('Summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f58ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detection from model name — no need to set provider manually\n",
    "from cofone.llm import _detect_provider\n",
    "for model, expected in [\n",
    "    ('arcee-ai/trinity-large-preview:free', 'openrouter'),\n",
    "    ('gpt-4o-mini',                         'openai'),\n",
    "    ('gemini-2.0-flash',                    'gemini'),\n",
    "]:\n",
    "    detected = _detect_provider(None, model, None)\n",
    "    print(f\"{'✓' if detected == expected else '✗'} {model!r:45s} → {detected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dad32",
   "metadata": {},
   "source": [
    "---\n",
    "## 7 · Sources — files, folders, PDF, URL, Wikipedia, YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad660ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single .txt file\n",
    "RAG().add_source(BASE / 'note_ex.txt').run('Summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder (recursively loads .txt .md .pdf)\n",
    "RAG().add_source(BASE / 'docs_ex/').run('Who is Leonardo?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia page\n",
    "RAG().debug().add_source('https://en.wikipedia.org/wiki/Artificial_intelligence').run('What is AI?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any web URL\n",
    "# RAG().add_source('https://example.com/article').run('Summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube transcript — first YouTube video ever\n",
    "RAG().debug().add_source('https://www.youtube.com/watch?v=jNQXAC9IVRw').run('What is this video about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF — requires: pip install pypdf\n",
    "# RAG().add_source('report.pdf').run('Summarize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ccee5b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8 · Multiple sources\n",
    "Chain as many `.add_source()` calls as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e7809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = (\n",
    "    RAG()\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .add_source(BASE / 'docs_ex/')\n",
    "    .run('What do you know about Cofone and Leonardo?')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04aef3",
   "metadata": {},
   "source": [
    "---\n",
    "## 9 · Chat memory\n",
    "`.chat()` keeps conversation history. Follow-up questions have full context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d802d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = RAG().add_source(BASE / 'docs_ex/')\n",
    "\n",
    "r1 = bot.chat('Who is Leonardo da Vinci?')\n",
    "print('Q1:', r1)\n",
    "\n",
    "r2 = bot.chat('When was he born?')  # knows 'he' = Leonardo\n",
    "print('Q2:', r2)\n",
    "\n",
    "r3 = bot.chat('What are his most famous paintings?')\n",
    "print('Q3:', r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481731e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset memory and start fresh\n",
    "bot.reset_memory()\n",
    "print(bot.chat('What are we talking about?'))  # no context anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory=True flag — same as using .chat() but explicit\n",
    "bot2 = RAG(memory=True).add_source(BASE / 'docs_ex/')\n",
    "print(bot2.run('Who is Leonardo?'))\n",
    "print(bot2.run('What did he paint?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336365e",
   "metadata": {},
   "source": [
    "---\n",
    "## 10 · Structured output (Pydantic)\n",
    "Get back a validated Python object instead of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efffb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    birth_year: int\n",
    "    nationality: str\n",
    "    most_famous_work: str\n",
    "\n",
    "data = RAG().add_source(BASE / 'docs_ex/').run('Extract data about Leonardo', schema=Person)\n",
    "\n",
    "print(f'name:             {data.name}')\n",
    "print(f'birth_year:       {data.birth_year}')\n",
    "print(f'nationality:      {data.nationality}')\n",
    "print(f'most_famous_work: {data.most_famous_work}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex schema example\n",
    "from typing import List\n",
    "\n",
    "class AISummary(BaseModel):\n",
    "    definition: str\n",
    "    main_subfields: List[str]\n",
    "    biggest_challenge: str\n",
    "\n",
    "result = (\n",
    "    RAG()\n",
    "    .add_source('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "    .run('Extract key info about AI', schema=AISummary)\n",
    ")\n",
    "print(result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b21429",
   "metadata": {},
   "source": [
    "---\n",
    "## 11 · Streaming\n",
    "Tokens arrive and print one by one — no waiting for the full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d71351",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG().add_source(BASE / 'docs_ex/')\n",
    "for token in rag.stream(\"Tell me about Leonardo's inventions\"):\n",
    "    print(token, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c258deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with debug\n",
    "rag2 = RAG().debug().add_source(BASE / 'note_ex.txt')\n",
    "for token in rag2.stream('Describe the chunking system'):\n",
    "    print(token, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda67695",
   "metadata": {},
   "source": [
    "---\n",
    "## 12 · Custom tools\n",
    "Attach functions the agent can use alongside the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c867b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(expression: str) -> str:\n",
    "    try:\n",
    "        return f'Result: {eval(expression)}'\n",
    "    except Exception as e:\n",
    "        return f'Error: {e}'\n",
    "\n",
    "def word_count(text: str) -> str:\n",
    "    return f'Word count: {len(text.split())}'\n",
    "\n",
    "answer = (\n",
    "    RAG()\n",
    "    .add_tool(calculate)\n",
    "    .add_tool(word_count)\n",
    "    .add_source(BASE / 'note_ex.txt')\n",
    "    .run('Summarize and tell me what is 144 / 12')\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28336113",
   "metadata": {},
   "source": [
    "---\n",
    "## 13 · API key — 3 ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way 1: direct parameter (highest priority)\n",
    "# RAG(api_key='sk-or-...').add_source(BASE / 'note_ex.txt').run('Summarize')\n",
    "\n",
    "# Way 2: .env file in project root\n",
    "# OPENROUTER_API_KEY=sk-or-...\n",
    "# OPENAI_API_KEY=sk-...\n",
    "# GEMINI_API_KEY=AI...\n",
    "\n",
    "# Way 3: system environment variable\n",
    "# $env:OPENROUTER_API_KEY='sk-or-...'    (PowerShell)\n",
    "# export OPENROUTER_API_KEY='sk-or-...'  (bash/zsh)\n",
    "\n",
    "print('Keys OK if tests above worked.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c22f07",
   "metadata": {},
   "source": [
    "---\n",
    "## RAG() — Full Parameter Reference\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|---|---|---|\n",
    "| `model` | provider default | LLM model string |\n",
    "| `provider` | `openrouter` | `openrouter` / `openai` / `gemini` / `ollama` |\n",
    "| `api_key` | `None` | API key — overrides .env |\n",
    "| `base_url` | provider default | Custom API endpoint |\n",
    "| `faiss` | `False` | Use FAISS semantic search |\n",
    "| `embedding_model` | `all-MiniLM-L6-v2` | sentence-transformers model |\n",
    "| `chunk_mode` | `smart` | `smart` / `paragraphs` / `sentences` / `fixed` |\n",
    "| `persist_path` | `None` | Folder path to save/load FAISS index |\n",
    "| `memory` | `False` | Enable chat memory (keeps history across `.run()` calls) |\n",
    "\n",
    "## Methods\n",
    "\n",
    "| Method | Returns | Description |\n",
    "|---|---|---|\n",
    "| `.add_source(path_or_url)` | `self` | Load file / folder / URL / YouTube |\n",
    "| `.add_tool(fn)` | `self` | Attach a custom function |\n",
    "| `.debug()` | `self` | Enable verbose logging |\n",
    "| `.run(query, schema=None)` | `str` or Pydantic model | Single query |\n",
    "| `.chat(query)` | `str` | Query with memory enabled |\n",
    "| `.stream(query)` | generator of `str` | Streaming query |\n",
    "| `.reset_memory()` | `self` | Clear conversation history |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
